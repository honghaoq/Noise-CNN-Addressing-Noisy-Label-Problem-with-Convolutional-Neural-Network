# Noise-CNN-Addressing-Noisy-Label-Problem-with-Convolutional-Neural-Network
Data collected in real world setting are often corruptedwith incorrect labels, such noise in training data can lead tosignificant performance degrade for models predictive ca-pability on test set if trained with noisy data.   To addressmodel performance degrade issue on noisy labels problem,we  propose  a  new  simple  yet  effective  method,  IterativeCross Majority Learning (ICML), which is to train multi-ple convolutional neural networks independently on noisydata, and update data label based on majority vote acrosspredicted outputs from the trained models.  We then repeatthe process in multiple stages iteratively,  and use the laststage trained CNNs to perform ensemble learning for mak-ing test set prediction.  Our experiments on MNIST datasetshows that the proposed ICML method is able to achievenear state of the art result of over 97% prediction accuracyon test set after trained on data with noise level as high as70%, and recovering over 90% of the noisy labels correctly.Moreover,  by  introducing  random  labeling  for  dis-agreedpredictions at early stages (i.e.  Iterative Cross Learning -Random,  or ICL-R), and applying ICML in later trainingstages,  we find such combination provides even better re-sult and converge faster than previous approaches. We alsoanalyzed the reasons of the effectiveness of this method inthe end of this paper.  Besides the effectiveness of proposedapproach in noisy label training, it can also be used for la-bel generation given a small portion of true label data (i.e.create random labels for unlabeled data and perform ICMLmethod to approximate true labels)
